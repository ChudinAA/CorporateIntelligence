# DeepSeek: R1 (free)Free variant

### [deepseek](https://openrouter.ai/deepseek)/deepseek-r1:free

[Chat](https://openrouter.ai/chat?models=deepseek/deepseek-r1:free) [Compare](https://openrouter.ai/compare/deepseek/deepseek-r1:free)

Created Jan 20, 2025163,840 context

$0/M input tokens$0/M output tokens

DeepSeek R1 is here: Performance on par with [OpenAI o1](https://openrouter.ai/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.

Fully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).

MIT licensed: Distill & commercialize freely!

[Chat](https://openrouter.ai/chat?models=deepseek/deepseek-r1:free) [Compare](https://openrouter.ai/compare/deepseek/deepseek-r1:free)

FreeFree variant [Model weights](https://huggingface.co/deepseek-ai/DeepSeek-R1)

Overview

Providers

Versions

Apps

Activity

Uptime

API

## Providers for R1 (free)

### OpenRouter [routes requests](https://openrouter.ai/docs/provider-routing) to the best providers that are able to handle your prompt size and parameters, with fallbacks to maximize [uptime](https://openrouter.ai/deepseek/deepseek-r1:free/uptime).

FilterSort by

|     |     |
| --- | --- |
| [Chutes](https://openrouter.ai/provider/chutes) | Context<br>164K<br>Max Output<br>164K<br>Input<br>$0<br>Output<br>$0<br>Latency<br>5.80s<br>Throughput<br>62.34t/s<br>Uptime |
| [Targon](https://openrouter.ai/provider/targon) | Context<br>164K<br>Max Output<br>164K<br>Input<br>$0<br>Output<br>$0<br>Latency<br>7.90s<br>Throughput<br>83.87t/s<br>Uptime |

### Throughput

### Latency

## Apps using R1 (free)

### Top public apps this week using this model

1.

![Favicon for https://sillytavern.app/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://sillytavern.app/&size=256)

SillyTavern

LLM frontend for power users

5.95Btokens

2.

![Favicon for https://cline.bot/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://cline.bot/&size=256)

Cline

Autonomous coding agent right in your IDE

4.47Btokens

3.

![Favicon for https://chub.ai/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://chub.ai/&size=256)

Chub AI

GenAI for everyone

4.47Btokens

4.

![Favicon for https://roocode.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://roocode.com/&size=256)

Roo Code

A whole dev team of AI agents in your editor

2.14Btokens

5.

![Favicon for https://quack.im/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://quack.im/&size=256)

Quack

new

627Mtokens

6.

![Favicon for https://agnai.chat/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://agnai.chat/&size=256)

Agnaistic

A "bring your own AI" chat service

546Mtokens

7.

![Favicon for https://openrouter.ai/chat](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://openrouter.ai/chat&size=256)

OpenRouter: Chatroom

Chat with multiple LLMs at once

539Mtokens

8.

![Favicon for https://chatboxai.app/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://chatboxai.app/&size=256)

Chatbox AI

new

481Mtokens

9.

![Favicon for https://app.wyvern.chat/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://app.wyvern.chat/&size=256)

WyvernChat

new

381Mtokens

10.

![Favicon for https://risuai.xyz/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://risuai.xyz/&size=256)

RisuAI

Browse characters, choose models, and chat

242Mtokens

11.

![Favicon for https://cherry-ai.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://cherry-ai.com/&size=256)

Cherry Studio

new

190Mtokens

12.

![Favicon for https://aider.chat/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://aider.chat/&size=256)

Aider

AI pair programming in your terminal

171Mtokens

13.

![Favicon for https://github.com/HybridTalentComputing/cline-chinese](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://github.com/HybridTalentComputing/cline-chinese&size=256)

Cline Chinese

new

127Mtokens

14.

![Favicon for https://openwebui.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://openwebui.com/&size=256)

Open WebUI

Extensible, self-hosted AI interface

66.3Mtokens

## Recent activity on R1 (free)

### Tokens processed per day

Jan 28Jan 31Feb 3Feb 6Feb 9Feb 12Feb 15Feb 18Feb 21Feb 24Feb 27Mar 2Mar 5Mar 8Mar 11Mar 14Mar 17Mar 20Mar 23Mar 26Mar 29Apr 103.5B7B10.5B14B

## Versions by Token Share

Mar 27Mar 28Mar 29Mar 30Mar 31Apr 1Apr 2Apr 30%30%60%100%

[DeepSeek: R1 Distill Llama 8B](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-8b)

Created February 7, 20250 context

248M tokens

[DeepSeek: R1 Distill Qwen 1.5B](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-1.5b)

Created January 31, 2025131,072 context

24.9M tokens

[DeepSeek: R1 Distill Qwen 32B](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-32b)

Created January 29, 2025128,000 context

513M tokens

[DeepSeek: R1 Distill Qwen 14B](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-14b)

Created January 29, 2025131,072 context

46.1M tokens

[DeepSeek: R1 Distill Llama 70B](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-70b)

Created January 23, 2025128,000 context

14.3B tokens

[DeepSeek: R1](https://openrouter.ai/deepseek/deepseek-r1)

Created January 20, 2025163,840 context

11.5B tokens

## Uptime stats for R1 (free)

### Uptime stats for R1 (free)across all providers

Datadog


#### Uptime

live

Percent7580859095100

17:0018:0019:0020:0021:0022:0023:00Thu 301:0002:0003:0004:0005:0006:0007:0008:0009:0010:0011:0012:0013:0014:0015:0016:00

Least Stable Provider

OpenRouter

Generated by [Datadog](http://www.datadoghq.com/)

When an error occurs in an upstream provider, we can recover by routing to another healthy provider, if your request filters allow it.

[Learn more](https://openrouter.ai/docs/provider-routing) about our load balancing and customization options.

Datadog


#### Successful Inference Finish Reasons

live

Percent0102030405060708090100110120

17:0018:0019:0020:0021:0022:0023:00Thu 301:0002:0003:0004:0005:0006:0007:0008:0009:0010:0011:0012:0013:0014:0015:0016:00

Successful

Generated by [Datadog](http://www.datadoghq.com/)

## Sample code and API for R1 (free)

### OpenRouter normalizes requests and responses across providers for you.

[Create API key](https://openrouter.ai/settings/keys)

OpenRouter provides an OpenAI-compatible completion API to 300+ models & providers that you can call directly, or using the OpenAI SDK. Additionally, some third-party SDKs are available.

In the examples below, the [OpenRouter-specific headers](https://openrouter.ai/docs/requests#request-headers) are optional. Setting them allows your app to appear on the OpenRouter leaderboards.

openai-pythonpythontypescriptopenai-typescriptcurl

Copy

```python
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  extra_body={},
  model="deepseek/deepseek-r1:free",
  messages=[\
    {\
      "role": "user",\
      "content": "What is the meaning of life?"\
    }\
  ]
)
print(completion.choices[0].message.content)
```

## Using third-party SDKs

For information about using third-party SDKs and frameworks with OpenRouter, please see our [frameworks documentation](https://openrouter.ai/docs/frameworks).

See the [Request docs](https://openrouter.ai/docs/api-reference/overview) for all possible fields, and [Parameters](https://openrouter.ai/docs/api-reference/parameters) for explanations of specific sampling parameters.

## More models from [DeepSeek](https://openrouter.ai/deepseek)

[DeepSeek V3 Base\\
\\
Note that this is a base model mostly meant for testing, you need to provide detailed prompts for the model to return useful responses.\\
\\
DeepSeek-V3 Base is a 671B parameter open Mixture-of-Experts (MoE) language model with 37B active parameters per forward pass and a context length of 128K tokens. Trained on 14.8T tokens using FP8 mixed precision, it achieves high training efficiency and stability, with strong performance across language, reasoning, math, and coding tasks.\\
\\
DeepSeek-V3 Base is the pre-trained model behind DeepSeek V3](https://openrouter.ai/deepseek/deepseek-v3-base)

[DeepSeek V3 0324\\
\\
DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\\
\\
It succeeds the DeepSeek V3 model and performs really well on a variety of tasks.](https://openrouter.ai/deepseek/deepseek-chat-v3-0324)

[DeepSeek R1 Zero\\
\\
DeepSeek-R1-Zero is a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step. It's 671B parameters in size, with 37B active in an inference pass.\\
\\
It demonstrates remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\\
\\
DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. See DeepSeek R1 for the SFT model.](https://openrouter.ai/deepseek/deepseek-r1-zero)

[R1 Distill Llama 8B\\
\\
DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, using outputs from DeepSeek R1. The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.\\
\\
Hugging Face:](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-8b)

[R1 Distill Qwen 1.5B\\
\\
DeepSeek R1 Distill Qwen 1.5B is a distilled large language model based on Qwen 2.5 Math 1.5B, using outputs from DeepSeek R1. It's a very small and efficient model which outperforms GPT 4o 0513 on Math Benchmarks.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-1.5b)

[R1 Distill Qwen 32B\\
\\
DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-32b)

[R1 Distill Qwen 14B\\
\\
DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-14b)

[R1 Distill Llama 70B\\
\\
DeepSeek R1 Distill Llama 70B is a distilled large language model based on Llama-3.3-70B-Instruct, using outputs from DeepSeek R1. The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-70b)

[R1\\
\\
DeepSeek R1 is here: Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\\
\\
Fully open-source model & technical report.\\
\\
MIT licensed: Distill & commercialize freely!](https://openrouter.ai/deepseek/deepseek-r1)

[DeepSeek V3\\
\\
DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\\
\\
For model details, please visit the DeepSeek-V3 repo for more information, or see the launch announcement.](https://openrouter.ai/deepseek/deepseek-chat)

[DeepSeek V2.5\\
\\
DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions. For model details, please visit DeepSeek-V2 page for more information.](https://openrouter.ai/deepseek/deepseek-chat-v2.5)

[DeepSeek-Coder-V2\\
\\
DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model. It is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens.\\
\\
The original V1 model was trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. It was pre-trained on project-level code corpus by employing a extra fill-in-the-blank task.](https://openrouter.ai/deepseek/deepseek-coder)

Previous slideNext slide

R1 (free) - API, Providers, Stats \| OpenRouter0%