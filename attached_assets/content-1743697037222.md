# DeepSeek: R1 (free)Free variant

### [deepseek](https://openrouter.ai/deepseek)/deepseek-r1:free

[Chat](https://openrouter.ai/chat?models=deepseek/deepseek-r1:free) [Compare](https://openrouter.ai/compare/deepseek/deepseek-r1:free)

Created Jan 20, 2025163,840 context

$0/M input tokens$0/M output tokens

DeepSeek R1 is here: Performance on par with [OpenAI o1](https://openrouter.ai/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.

Fully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).

MIT licensed: Distill & commercialize freely!

[Chat](https://openrouter.ai/chat?models=deepseek/deepseek-r1:free) [Compare](https://openrouter.ai/compare/deepseek/deepseek-r1:free)

FreeFree variant [Model weights](https://huggingface.co/deepseek-ai/DeepSeek-R1)

Overview

Providers

Versions

Apps

Activity

Uptime

API

## Sample code and API for R1 (free)

### OpenRouter normalizes requests and responses across providers for you.

[Create API key](https://openrouter.ai/settings/keys)

OpenRouter provides an OpenAI-compatible completion API to 300+ models & providers that you can call directly, or using the OpenAI SDK. Additionally, some third-party SDKs are available.

In the examples below, the [OpenRouter-specific headers](https://openrouter.ai/docs/requests#request-headers) are optional. Setting them allows your app to appear on the OpenRouter leaderboards.

openai-pythonpythontypescriptopenai-typescriptcurl

Copy

```python
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  extra_body={},
  model="deepseek/deepseek-r1:free",
  messages=[\
    {\
      "role": "user",\
      "content": "What is the meaning of life?"\
    }\
  ]
)
print(completion.choices[0].message.content)
```

## Using third-party SDKs

For information about using third-party SDKs and frameworks with OpenRouter, please see our [frameworks documentation](https://openrouter.ai/docs/frameworks).

See the [Request docs](https://openrouter.ai/docs/api-reference/overview) for all possible fields, and [Parameters](https://openrouter.ai/docs/api-reference/parameters) for explanations of specific sampling parameters.

## More models from [DeepSeek](https://openrouter.ai/deepseek)

[DeepSeek V3 Base\\
\\
Note that this is a base model mostly meant for testing, you need to provide detailed prompts for the model to return useful responses.\\
\\
DeepSeek-V3 Base is a 671B parameter open Mixture-of-Experts (MoE) language model with 37B active parameters per forward pass and a context length of 128K tokens. Trained on 14.8T tokens using FP8 mixed precision, it achieves high training efficiency and stability, with strong performance across language, reasoning, math, and coding tasks.\\
\\
DeepSeek-V3 Base is the pre-trained model behind DeepSeek V3](https://openrouter.ai/deepseek/deepseek-v3-base)

[DeepSeek V3 0324\\
\\
DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\\
\\
It succeeds the DeepSeek V3 model and performs really well on a variety of tasks.](https://openrouter.ai/deepseek/deepseek-chat-v3-0324)

[DeepSeek R1 Zero\\
\\
DeepSeek-R1-Zero is a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step. It's 671B parameters in size, with 37B active in an inference pass.\\
\\
It demonstrates remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\\
\\
DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. See DeepSeek R1 for the SFT model.](https://openrouter.ai/deepseek/deepseek-r1-zero)

[R1 Distill Llama 8B\\
\\
DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, using outputs from DeepSeek R1. The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.\\
\\
Hugging Face:](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-8b)

[R1 Distill Qwen 1.5B\\
\\
DeepSeek R1 Distill Qwen 1.5B is a distilled large language model based on Qwen 2.5 Math 1.5B, using outputs from DeepSeek R1. It's a very small and efficient model which outperforms GPT 4o 0513 on Math Benchmarks.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-1.5b)

[R1 Distill Qwen 32B\\
\\
DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-32b)

[R1 Distill Qwen 14B\\
\\
DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\
\\
Other benchmark results include:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-14b)

[R1 Distill Llama 70B\\
\\
DeepSeek R1 Distill Llama 70B is a distilled large language model based on Llama-3.3-70B-Instruct, using outputs from DeepSeek R1. The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\\
\\
The model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-70b)

[R1\\
\\
DeepSeek R1 is here: Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\\
\\
Fully open-source model & technical report.\\
\\
MIT licensed: Distill & commercialize freely!](https://openrouter.ai/deepseek/deepseek-r1)

[DeepSeek V3\\
\\
DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\\
\\
For model details, please visit the DeepSeek-V3 repo for more information, or see the launch announcement.](https://openrouter.ai/deepseek/deepseek-chat)

[DeepSeek V2.5\\
\\
DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions. For model details, please visit DeepSeek-V2 page for more information.](https://openrouter.ai/deepseek/deepseek-chat-v2.5)

[DeepSeek-Coder-V2\\
\\
DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model. It is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens.\\
\\
The original V1 model was trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. It was pre-trained on project-level code corpus by employing a extra fill-in-the-blank task.](https://openrouter.ai/deepseek/deepseek-coder)

Previous slideNext slide

DeepSeek: R1 (free) â€“ Run with an API \| OpenRouter